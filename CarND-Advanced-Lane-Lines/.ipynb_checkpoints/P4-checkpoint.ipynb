{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Advanced Lane Lines Detection on the Road** \n",
    "***\n",
    "In this project, we write a software pipeline to identify the lane boundaries in a video from a front-facing camera on a car. \n",
    "\n",
    "## **Overview**\n",
    "\n",
    "Here is just an over view of the pipeline. We provide details below.\n",
    "\n",
    "<p align=\"center\">\n",
    " <img src=\"./output_images/line_detection_pipeline.png\">\n",
    "</p>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Camera Calibration**\n",
    "\n",
    "We calibrate the camera using the example images provided in `/camera_cal/calibration*.jpg`. These are chessboard images. \n",
    "\n",
    "1. We convert image to gray scale using `cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)`\n",
    "1. We start by preparing object points, which will be the (x, y, z) coordinates of the chessboard corners in the world. We prepare object points like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0) such that the object points are the same for each calibration image.\n",
    "2. Every time we successfully detect all chessboard corners in a test image, imgpoints will be appended with the corners returned by `cv2.findChessboardCorners`.\n",
    "\n",
    "We then use these object and image points to compute the camera calibration and distortion coefficients using the cv2.calibrateCamera() function.\n",
    "\n",
    "Example of undistortion from camera calibration\n",
    "\n",
    "<p align=\"center\">\n",
    " <img src=\"./output_images/chessboard_original_undistorted.png\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import pickle\n",
    "import glob\n",
    "from moviepy.editor import VideoFileClip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# globals\n",
    "mtx = None\n",
    "dist = None\n",
    "M = None\n",
    "Minv = None\n",
    "line_prev = None\n",
    "num_full_search = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Display util methods.\n",
    "def display(img, tag1, undist, tag2):\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 9))\n",
    "    f.tight_layout()\n",
    "    ax1.imshow(img)\n",
    "    ax1.set_title(tag1, fontsize=50)\n",
    "    ax2.imshow(undist)\n",
    "    ax2.set_title(tag2, fontsize=50)\n",
    "    plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_corners(images):\n",
    "    # prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)\n",
    "    objp = np.zeros((6*9,3), np.float32)\n",
    "    objp[:,:2] = np.mgrid[0:9, 0:6].T.reshape(-1,2)\n",
    "\n",
    "    # Arrays to store object points and image points from all the images.\n",
    "    objpoints = [] # 3d points in real world space\n",
    "    imgpoints = [] # 2d points in image plane.\n",
    "    \n",
    "    image_dims = None\n",
    "    # Step through the list and search for chessboard corners\n",
    "    for idx, fname in enumerate(images):\n",
    "        img = cv2.imread(fname)\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Find the chessboard corners\n",
    "        ret, corners = cv2.findChessboardCorners(gray, (9,6), None)\n",
    "\n",
    "        # If found, add object points, image points\n",
    "        if ret == True:\n",
    "            objpoints.append(objp)\n",
    "            imgpoints.append(corners)\n",
    "            image_dims = (img.shape[0], img.shape[1])\n",
    "\n",
    "    return objpoints, imgpoints, image_dims\n",
    "\n",
    "def calibrate_camera(path):\n",
    "    images = glob.glob(path)\n",
    "    objpoints, imgpoints, image_dims = find_corners(images)\n",
    "    ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, image_dims, None, None)\n",
    "    return mtx, dist\n",
    "\n",
    "def cal_undistort(img, mtx, dist):\n",
    "    undist = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "    return undist\n",
    "\n",
    "mtx, dist = calibrate_camera(\"./camera_cal/calibration*.jpg\")\n",
    "img = cv2.imread('./camera_cal/calibration1.jpg')\n",
    "undist_img = cal_undistort(img, mtx, dist)\n",
    "display(img, 'Original', undist_img, 'Undistorted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Pipeline (test images)**\n",
    "\n",
    "### **Distortion Correction**\n",
    "\n",
    "We apply distortion correction to one of the test images. The differences are subtle and most noticeable around the edges.\n",
    "\n",
    "<p align=\"center\">\n",
    " <img src=\"./output_images/original_undistorted.png\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('./camera_cal/distorted.png')\n",
    "undist_img = cal_undistort(img, mtx, dist)\n",
    "display(img, 'Original', undist_img, 'Undistorted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Binary Thresholding**\n",
    "\n",
    "We do combined thresholding using two different techniques.\n",
    "\n",
    "1. Sobel X\n",
    "2. Thresholding in HLS color space.\n",
    "\n",
    "We take the input RGB image, convert it to grayscale. We then apply a Sobel filter in the X direction to get image edges that match the direction of the lane lines.  Taking the gradient in the x-direction emphasizes edges closer to vertical. We then apply a threshold function on this to filter out out pixels that are not of interest. Through trial and error, we settled on min/max threshold values of 30 and 150.\n",
    "\n",
    "We convert the RGB image to the HLS color space, and then use the S channel. The S saturation channel is useful for picking out lane lines under different color and contrast conditions, such as shadows. Through experiments, we found values of 175 and 250 to work best here.\n",
    "\n",
    "<p align=\"center\">\n",
    " <img src=\"./output_images/thresholding_output.png\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def abs_sobel_thresh(gray, orient='x', sobel_kernel=3, thresh=(0, 255)):\n",
    "    \n",
    "    # Apply the following steps to img\n",
    "    # 1) Convert to grayscale\n",
    "    # 2) Take the derivative in x or y given orient = 'x' or 'y'\n",
    "    # 3) Take the absolute value of the derivative or gradient\n",
    "    # 4) Scale to 8-bit (0 - 255) then convert to type = np.uint8\n",
    "    # 5) Create a mask of 1's where the scaled gradient magnitude \n",
    "            # is > thresh_min and < thresh_max\n",
    "    # 6) Return this mask as your binary_output image\n",
    "    sobel = None\n",
    "    if orient == 'x':\n",
    "        sobel = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=sobel_kernel)\n",
    "    else:\n",
    "        sobel = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=sobel_kernel)\n",
    "    abs_sobel = np.absolute(sobel)\n",
    "    scaled_sobel = np.uint8(255*abs_sobel/np.max(abs_sobel))\n",
    "    binary_output = np.zeros_like(scaled_sobel)\n",
    "    binary_output[(scaled_sobel >= thresh[0]) & (scaled_sobel <= thresh[1])] = 1\n",
    "    return binary_output\n",
    "\n",
    "def binary_thresholding(img, s_thresh=(175, 250), sx_thresh=(30, 150)):\n",
    "    # Convert to HSV color space and separate the V channel\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HLS).astype(np.float)\n",
    "    l_channel = hsv[:,:,1]\n",
    "    s_channel = hsv[:,:,2]\n",
    "    # Sobel\n",
    "    ksize = 3\n",
    "    sobel = abs_sobel_thresh(gray, orient='x', sobel_kernel=ksize, thresh=sx_thresh)\n",
    "    # Threshold color channel\n",
    "    s_binary = np.zeros_like(s_channel)\n",
    "    s_binary[(s_channel >= s_thresh[0]) & (s_channel <= s_thresh[1])] = 1\n",
    "    combined = np.zeros_like(gray)\n",
    "    combined[(s_binary == 1) | (sobel == 1)] = 1\n",
    "    return combined\n",
    "\n",
    "s_thresh = (175, 250)\n",
    "sx_thresh = (30, 150)\n",
    "binary_threshold = binary_thresholding(undist_img, s_thresh=s_thresh, sx_thresh=sx_thresh)\n",
    "display(undist_img, 'Original', binary_threshold, 'Combined Binary Thresholding')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Perspective Transform**\n",
    "\n",
    "Once we have the binary threshold image above, we apply a perspective transform on the image to generate an image with the effect of looking down on the road from above. OpenCV warpPerspective() function is used to do this. Through some experiements (and based on some of the CarND slack channel discussions) we settled on following coordinates:\n",
    "\n",
    "1. Source - [220,719],[1220,719],[750,480],[550,480]\n",
    "2. Destination - [240,719],[1040,719],[1040,300],[240,300]\n",
    "\n",
    "OpenCV getPerspectiveTransform() function generates a perspective matrix using these coordinates. After we have applied the perspective transform to the binary threshold image, we get an image that looks that lanes from above, like so.\n",
    "\n",
    "<p align=\"center\">\n",
    " <img src=\"./output_images/perspective_transform.png\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def perspective_transform(img):\n",
    "    # define 4 source points for perspective transformation\n",
    "    src = np.float32([[220,719],[1220,719],[750,480],[550,480]])\n",
    "    # define 4 destination points for perspective transformation\n",
    "    dst = np.float32([[240,719],[1040,719],[1040,300],[240,300]])\n",
    "    # Given src and dst points, calculate the perspective transform matrix\n",
    "    M = cv2.getPerspectiveTransform(src, dst)\n",
    "    # Warp the image using OpenCV warpPerspective()\n",
    "    warped = cv2.warpPerspective(img, M, (img.shape[1], img.shape[0]))\n",
    "    # Return the resulting image\n",
    "    return warped, M\n",
    "\n",
    "binary_warped, M = perspective_transform(binary_threshold)\n",
    "display(binary_threshold, 'Thresholded', binary_warped, 'Perspective Transform')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Locate Lines and Polynomial fit**\n",
    "\n",
    "In this stage is where we extract the actual lane pixels for both the left and right lanes from the above perspective transformed (bird's eye view) image. We use the peak histogram technique described in the lectures. We also borrow some code from the lectures. There are two ways of locating lines in an image. \n",
    "\n",
    "1. Naive approach (we brute force search across the image using the histogram technique)\n",
    "2. Smart approach (we use the line location from the previous image)\n",
    "\n",
    "We start with (1) and resort to (1) when we get \"lost\". We will cover what it means to be lost below. We first take a histogram along all the columns in the lower half of the image. With this histogram we are adding up the pixel values along each column in the image. In thresholded binary image, pixels are either 0 or 1, so the two most prominent peaks in this histogram will be good indicators of the x-position of the base of the lane lines. We can use that as a starting point for where to search for the lines. From that point onwards, we use a sliding window, placed around the line centers, to find and follow the lines up to the top of the frame. The following image shows how this search works.\n",
    "\n",
    "<p align=\"center\">\n",
    " <img src=\"./output_images/naive_locate_lines.png\">\n",
    "</p>\n",
    "\n",
    "In (2), when we use the smart approach and pick up the search from around the lines detected in the previous frame instead of searching all across the bottom of the image for lane start. It looks like this.\n",
    "\n",
    "<p align=\"center\">\n",
    " <img src=\"./output_images/smart_locate_lines.png\">\n",
    "</p>\n",
    "\n",
    "Once we have detected the lane pixes (for both left and right lanes) we use `numpy.polyfit` to fit a polynomial for these pixel positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Line():\n",
    "    # We use this class to hold the details of the located lines and polynomial fit for each image.\n",
    "    def __init__(self):\n",
    "        self.fullsearch = False  \n",
    "        self.left_lane_inds = None \n",
    "        self.right_lane_inds = None   \n",
    "        self.left_fit = None  \n",
    "        self.right_fit = None \n",
    "        self.left_fit_cr = None  \n",
    "        self.right_fit_cr = None \n",
    "        self.yvals = None\n",
    "        self.left_fitx = None\n",
    "        self.right_fitx = None\n",
    "        self.y_bottom = None\n",
    "        self.y_top = None\n",
    "        self.left_x_bottom = None\n",
    "        self.left_x_top = None\n",
    "        self.right_x_bottom = None\n",
    "        self.right_x_top = None\n",
    "        self.left_curverads = None\n",
    "        self.right_curverads = None\n",
    "        self.mean_left_curverad = None\n",
    "        self.mean_right_curverad = None\n",
    "\n",
    "def display_poly_fit(binary_warped, left_lane_inds, right_lane_inds, out_img, plotSearchArea=False):\n",
    "    left_fit, right_fit = fit_curve(binary_warped, left_lane_inds, right_lane_inds)\n",
    "    nonzero = binary_warped.nonzero()\n",
    "    nonzeroy = np.array(nonzero[0])\n",
    "    nonzerox = np.array(nonzero[1])\n",
    "    margin = 100\n",
    "    # Generate x and y values for plotting\n",
    "    fity = np.linspace(0, binary_warped.shape[0]-1, binary_warped.shape[0] )\n",
    "    fit_leftx = left_fit[0]*fity**2 + left_fit[1]*fity + left_fit[2]\n",
    "    fit_rightx = right_fit[0]*fity**2 + right_fit[1]*fity + right_fit[2]\n",
    "    # Color in left and right line pixels\n",
    "    out_img[nonzeroy[left_lane_inds], nonzerox[left_lane_inds]] = [255, 0, 0]\n",
    "    out_img[nonzeroy[right_lane_inds], nonzerox[right_lane_inds]] = [0, 0, 255]\n",
    "\n",
    "    result = None\n",
    "    if plotSearchArea == True:\n",
    "        window_img = np.zeros_like(out_img)\n",
    "        # Generate a polygon to illustrate the search window area\n",
    "        # And recast the x and y points into usable format for cv2.fillPoly()\n",
    "        left_line_window1 = np.array([np.transpose(np.vstack([fit_leftx-margin, fity]))])\n",
    "        left_line_window2 = np.array([np.flipud(np.transpose(np.vstack([fit_leftx+margin, fity])))])\n",
    "        left_line_pts = np.hstack((left_line_window1, left_line_window2))\n",
    "        right_line_window1 = np.array([np.transpose(np.vstack([fit_rightx-margin, fity]))])\n",
    "        right_line_window2 = np.array([np.flipud(np.transpose(np.vstack([fit_rightx+margin, fity])))])\n",
    "        right_line_pts = np.hstack((right_line_window1, right_line_window2))\n",
    "\n",
    "        # Draw the lane onto the warped blank image\n",
    "        cv2.fillPoly(window_img, np.int_([left_line_pts]), (0,255, 0))\n",
    "        cv2.fillPoly(window_img, np.int_([right_line_pts]), (0,255, 0))\n",
    "        result = cv2.addWeighted(out_img, 1, window_img, 0.3, 0)\n",
    "    else:\n",
    "        result = out_img\n",
    "\n",
    "    plt.imshow(result)\n",
    "    plt.plot(fit_leftx, fity, color='yellow')\n",
    "    plt.plot(fit_rightx, fity, color='yellow')\n",
    "    plt.xlim(0, 1280)\n",
    "    plt.ylim(720, 0)\n",
    "    plt.show()\n",
    "    \n",
    "def draw_lines(undist, warped, yvals, left_fitx, right_fitx, Minv):\n",
    "    # Create an image to draw the lines on\n",
    "    warp_zero = np.zeros_like(warped).astype(np.uint8)\n",
    "    color_warp = np.dstack((warp_zero, warp_zero, warp_zero))\n",
    "\n",
    "    # Recast the x and y points into usable format for cv2.fillPoly()\n",
    "    pts_left = np.array([np.transpose(np.vstack([left_fitx, yvals]))])\n",
    "    pts_right = np.array([np.flipud(np.transpose(np.vstack([right_fitx, yvals])))])\n",
    "    pts = np.hstack((pts_left, pts_right))\n",
    "\n",
    "    # Draw the lane onto the warped blank image\n",
    "    cv2.fillPoly(color_warp, np.int_([pts]), (0,255, 0))\n",
    "\n",
    "    # Warp the blank back to original image space using inverse perspective matrix (mtxinv)\n",
    "    newwarp = cv2.warpPerspective(color_warp, Minv, (color_warp.shape[1], color_warp.shape[0])) \n",
    "    # Combine the result with the original image\n",
    "    result = cv2.addWeighted(undist, 1, newwarp, 0.3, 0)\n",
    "    return result\n",
    "\n",
    "def naive_find_lines(binary_warped):\n",
    "    # Take a histogram of the bottom half of the image\n",
    "    histogram = np.sum(binary_warped[binary_warped.shape[0]/2:,:], axis=0)\n",
    "    # Create an output image to draw on and  visualize the result\n",
    "    out_img = np.dstack((binary_warped, binary_warped, binary_warped))*255\n",
    "    # Find the peak of the left and right halves of the histogram\n",
    "    # These will be the starting point for the left and right lines\n",
    "    midpoint = np.int(histogram.shape[0]/2)\n",
    "    leftx_base = np.argmax(histogram[:midpoint])\n",
    "    rightx_base = np.argmax(histogram[midpoint:]) + midpoint\n",
    "\n",
    "    # Choose the number of sliding windows\n",
    "    nwindows = 9\n",
    "    # Set height of windows\n",
    "    window_height = np.int(binary_warped.shape[0]/nwindows)\n",
    "    # Identify the x and y positions of all nonzero pixels in the image\n",
    "    nonzero = binary_warped.nonzero()\n",
    "    nonzeroy = np.array(nonzero[0])\n",
    "    nonzerox = np.array(nonzero[1])\n",
    "    # Current positions to be updated for each window\n",
    "    leftx_current = leftx_base\n",
    "    rightx_current = rightx_base\n",
    "    # Set the width of the windows +/- margin\n",
    "    margin = 100\n",
    "    # Set minimum number of pixels found to recenter window\n",
    "    minpix = 50\n",
    "    # Create empty lists to receive left and right lane pixel indices\n",
    "    left_lane_inds = []\n",
    "    right_lane_inds = []\n",
    "\n",
    "    # Step through the windows one by one\n",
    "    for window in range(nwindows):\n",
    "        # Identify window boundaries in x and y (and right and left)\n",
    "        win_y_low = binary_warped.shape[0] - (window+1)*window_height\n",
    "        win_y_high = binary_warped.shape[0] - window*window_height\n",
    "        win_xleft_low = leftx_current - margin\n",
    "        win_xleft_high = leftx_current + margin\n",
    "        win_xright_low = rightx_current - margin\n",
    "        win_xright_high = rightx_current + margin\n",
    "        # Draw the windows on the visualization image\n",
    "        cv2.rectangle(out_img,(win_xleft_low,win_y_low),(win_xleft_high,win_y_high),(0,255,0), 2) \n",
    "        cv2.rectangle(out_img,(win_xright_low,win_y_low),(win_xright_high,win_y_high),(0,255,0), 2) \n",
    "        # Identify the nonzero pixels in x and y within the window\n",
    "        good_left_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & (nonzerox >= win_xleft_low) & (nonzerox < win_xleft_high)).nonzero()[0]\n",
    "        good_right_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & (nonzerox >= win_xright_low) & (nonzerox < win_xright_high)).nonzero()[0]\n",
    "        # Append these indices to the lists\n",
    "        left_lane_inds.append(good_left_inds)\n",
    "        right_lane_inds.append(good_right_inds)\n",
    "        # If you found > minpix pixels, recenter next window on their mean position\n",
    "        if len(good_left_inds) > minpix:\n",
    "            leftx_current = np.int(np.mean(nonzerox[good_left_inds]))\n",
    "        if len(good_right_inds) > minpix:        \n",
    "            rightx_current = np.int(np.mean(nonzerox[good_right_inds]))\n",
    "\n",
    "    # Concatenate the arrays of indices\n",
    "    left_lane_inds = np.concatenate(left_lane_inds)\n",
    "    right_lane_inds = np.concatenate(right_lane_inds)\n",
    "\n",
    "    return left_lane_inds, right_lane_inds, out_img\n",
    "\n",
    "def smart_find_lines(binary_warped, left_fit, right_fit):\n",
    "    nonzero = binary_warped.nonzero()\n",
    "    nonzeroy = np.array(nonzero[0])\n",
    "    nonzerox = np.array(nonzero[1])\n",
    "\n",
    "    margin = 100\n",
    "    left_lane_inds = ((nonzerox > (left_fit[0]*(nonzeroy**2) + \n",
    "        left_fit[1]*nonzeroy + left_fit[2] - margin)) & \n",
    "    (nonzerox < (left_fit[0]*(nonzeroy**2) + left_fit[1]*nonzeroy + left_fit[2] + margin))) \n",
    "    right_lane_inds = ((nonzerox > (right_fit[0]*(nonzeroy**2) + \n",
    "        right_fit[1]*nonzeroy + right_fit[2] - margin)) & \n",
    "    (nonzerox < (right_fit[0]*(nonzeroy**2) + right_fit[1]*nonzeroy + right_fit[2] + margin)))  \n",
    "\n",
    "    # Again, extract left and right line pixel positions\n",
    "    leftx = nonzerox[left_lane_inds]\n",
    "    lefty = nonzeroy[left_lane_inds] \n",
    "    rightx = nonzerox[right_lane_inds]\n",
    "    righty = nonzeroy[right_lane_inds]\n",
    "    # Fit a second order polynomial to each\n",
    "    left_fit_new = np.polyfit(lefty, leftx, 2)\n",
    "    right_fit_new = np.polyfit(righty, rightx, 2)\n",
    "\n",
    "    # Create an image to draw on and an image to show the selection window\n",
    "    out_img = np.dstack((binary_warped, binary_warped, binary_warped))*255\n",
    "    return left_lane_inds, right_lane_inds, out_img    \n",
    "\n",
    "def fit_curve(binary_warped, left_lane_inds, right_lane_inds):\n",
    "    nonzero = binary_warped.nonzero()\n",
    "    nonzeroy = np.array(nonzero[0])\n",
    "    nonzerox = np.array(nonzero[1])\n",
    "    # Extract left and right line pixel positions\n",
    "    leftx = nonzerox[left_lane_inds]\n",
    "    lefty = nonzeroy[left_lane_inds] \n",
    "    rightx = nonzerox[right_lane_inds]\n",
    "    righty = nonzeroy[right_lane_inds] \n",
    "    # Fit a second order polynomial to each\n",
    "    left_fit = np.polyfit(lefty, leftx, 2)\n",
    "    right_fit = np.polyfit(righty, rightx, 2)\n",
    "    return left_fit, right_fit\n",
    "\n",
    "def process_fit(binary_warped, left_lane_inds, right_lane_inds):\n",
    "    left_fit, right_fit = fit_curve(binary_warped, left_lane_inds, right_lane_inds)\n",
    "    left_fit_cr, right_fit_cr = fit_pixel_to_meters(binary_warped, left_lane_inds, right_lane_inds)\n",
    "    yvals, left_fitx, right_fitx = fit_lines(binary_warped, left_fit, right_fit)\n",
    "\n",
    "    line = Line()\n",
    "    line.left_lane_inds = left_lane_inds\n",
    "    line.right_lane_inds = right_lane_inds\n",
    "    line.left_fit = left_fit\n",
    "    line.right_fit = right_fit\n",
    "    line.left_fit_cr = left_fit_cr\n",
    "    line.right_fit_cr = right_fit_cr\n",
    "    line.yvals = yvals\n",
    "    line.left_fitx = left_fitx\n",
    "    line.right_fitx = right_fitx\n",
    "    line.y_bottom = np.min(yvals)\n",
    "    line.y_top = np.max(yvals)\n",
    "    line.left_x_bottom = left_fit[0]*line.y_bottom**2 + left_fit[1]*line.y_bottom + left_fit[2]\n",
    "    line.left_x_top = left_fit[0]*line.y_top**2 + left_fit[1]*line.y_top + left_fit[2]\n",
    "    line.right_x_bottom = right_fit[0]*line.y_bottom**2 + right_fit[1]*line.y_bottom + right_fit[2]\n",
    "    line.right_x_top = right_fit[0]*line.y_top**2 + right_fit[1]*line.y_top + right_fit[2]\n",
    "    left_curverads, right_curverads = radius_of_curvatures(line.yvals, left_fit_cr, right_fit_cr)\n",
    "    line.left_curverads = left_curverads\n",
    "    line.right_curverads = right_curverads\n",
    "    line.mean_left_curverad = np.mean(left_curverads)\n",
    "    line.mean_right_curverad = np.mean(right_curverads)\n",
    "\n",
    "    return line\n",
    "\n",
    "def fit_lines(binary_warped, left_fit, right_fit):\n",
    "    yvals = np.linspace(0, binary_warped.shape[0]-1, binary_warped.shape[0] )\n",
    "    left_fitx = left_fit[0]*yvals**2 + left_fit[1]*yvals + left_fit[2]\n",
    "    right_fitx = right_fit[0]*yvals**2 + right_fit[1]*yvals + right_fit[2]\n",
    "    return yvals, left_fitx, right_fitx\n",
    "\n",
    "# naive search\n",
    "left_lane_inds, right_lane_inds, out_img = naive_find_lines(binary_warped)\n",
    "line = process_fit(binary_warped, left_lane_inds, right_lane_inds)\n",
    "display_poly_fit(binary_warped, left_lane_inds, right_lane_inds, out_img, False)\n",
    "\n",
    "# smart search (we are faking it here)\n",
    "left_lane_inds, right_lane_inds, out_img = smart_find_lines(binary_warped, line.left_fit, line.right_fit)\n",
    "line = process_fit(binary_warped, left_lane_inds, right_lane_inds)\n",
    "display_poly_fit(binary_warped, left_lane_inds, right_lane_inds, out_img, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Curvature of the lane and vehicle position with respect to center**\n",
    "\n",
    "Next we'll compute the radius of curvature of the poly fit done above. However that polynomial fit is in pixel space. In order to calculate the curvature in real space (in meters) we convert pixels to meters and do the polynomial fit. It is just a simple conversion of units. So we actually need to repeat this calculation after converting our x and y values to real world space. This involves measuring how long and wide the section of lane is that we're projecting in our warped image. We could do this in detail by measuring out the physical lane in the field of view of the camera, but for this project, we use the values provided in lectures. The lane is about 30 meters long and 3.7 meters wide. So on y dimension a pixel is 30/720 meters and on x dimension, a pixel is 3.7/700 meters. We use the mean curvature across the line.\n",
    "\n",
    "To calculate the offset from centre, we use the bottom x values for the lanes, i.e, where on the x plane, both the left and right lanes crossed the image near the driver. We then calculate the center of the image as the width/2. The offset was calculated as `(rx - xcenter) - (xcenter - lx)` and converted to meters using the pixel to meters conversion stated above.\n",
    "\n",
    "The result of all this, once plotted back on undistorted image (located lines drawn based on poly fit and projected back from warped perspective) looks like this:\n",
    "\n",
    "<p align=\"center\">\n",
    " <img src=\"./output_images/annotated_result.png\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fit_pixel_to_meters(binary_warped, left_lane_inds, right_lane_inds):\n",
    "    nonzero = binary_warped.nonzero()\n",
    "    nonzeroy = np.array(nonzero[0])\n",
    "    nonzerox = np.array(nonzero[1])\n",
    "    # Again, extract left and right line pixel positions\n",
    "    leftx = nonzerox[left_lane_inds]\n",
    "    lefty = nonzeroy[left_lane_inds] \n",
    "    rightx = nonzerox[right_lane_inds]\n",
    "    righty = nonzeroy[right_lane_inds]\n",
    "    # Define conversions in x and y from pixels space to meters\n",
    "    ym_per_pix = 30/720 # meters per pixel in y dimension\n",
    "    xm_per_pix = 3.7/700 # meters per pixel in x dimension\n",
    "    # Fit new polynomials to x,y in world space\n",
    "    left_fit_cr = np.polyfit(lefty*ym_per_pix, leftx*xm_per_pix, 2)\n",
    "    right_fit_cr = np.polyfit(righty*ym_per_pix, rightx*xm_per_pix, 2)\n",
    "    return left_fit_cr, right_fit_cr\n",
    "\n",
    "def radius_of_curvatures(yvals, left_fit, right_fit):\n",
    "    left_curverads = ((1 + (2*left_fit[0]*yvals + left_fit[1])**2)**1.5) / np.absolute(2*left_fit[0])\n",
    "    right_curverads = ((1 + (2*right_fit[0]*yvals + right_fit[1])**2)**1.5) / np.absolute(2*right_fit[0])\n",
    "    return left_curverads, right_curverads\n",
    "\n",
    "def annotate_result(result, line):\n",
    "    lx = line.left_x_top\n",
    "    rx = line.right_x_top\n",
    "    xcenter = np.int(result.shape[1]/2)\n",
    "    offset = (rx - xcenter) - (xcenter - lx) \n",
    "    xm_per_pix = 3.7/700 # meters per pixel in x dimension\n",
    "    vehicle_offset =  offset * xm_per_pix\n",
    "\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    cv2.putText(result, 'Mean Radius of curvature (Left)  = %.2f m' % (line.mean_left_curverad), \n",
    "        (10, 40), font, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    cv2.putText(result, 'Mean Radius of curvature (Right) = %.2f m' % (line.mean_left_curverad), \n",
    "        (10, 70), font, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    cv2.putText(result, 'Vehicle position = %.2f m from lane center' % (vehicle_offset), \n",
    "               (10, 100), font, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    return result\n",
    "\n",
    "result = draw_lines(undist_img, binary_warped, line.yvals, line.left_fitx, line.right_fitx, np.linalg.inv(M))\n",
    "annotated_result = annotate_result(result, line)\n",
    "plt.imshow(result)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Why Smart ?**\n",
    "\n",
    "Doing the naive search on each frame is expensive. If we assume the camera is capturing 30 frames a second, there should be no reason why we cannot leverage the lane location from previous frame on to next frame.\n",
    "\n",
    "Once we have calculated for a given frame:\n",
    "\n",
    "1) X positions where lanes intersect the bottom of the frame.\n",
    "2) Mean radius of curvature of lanes\n",
    "\n",
    "For (1), we can check that these have not changed too much from frame to frame. If you think about it, at 30 frames per second from a camera, the lanes should not change too much from frame to frame. We check for a 15 pixel delta.\n",
    "\n",
    "For (2), we check that each lanes Radius of Curvature is within 100 times (larger or smaller) than the previous frames values. As the RoC can be very large for vertical lines, so 100x check seems to work.\n",
    "\n",
    "If any of the above checks fail for a lane, I consider the lane ‘lost’ and we do a naive search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Video Pipeline**\n",
    "\n",
    "Putting it all together, we use `moviepy` software to run the pipeline on each image. You can see the output video here: (https://youtu.be/MuK4aSims0Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def is_good_fit(prev, curr):\n",
    "    # check if left_x_bottom and right_x_bottom are within 15 pixels\n",
    "    if abs(prev.left_x_bottom - curr.left_x_bottom) <= 15: \n",
    "        if abs(prev.right_x_bottom - curr.right_x_bottom) <= 15:\n",
    "                if abs(curr.mean_left_curverad) < (abs(prev.mean_left_curverad*100)):\n",
    "                    if abs(curr.mean_right_curverad) < (abs(prev.mean_left_curverad*100)):\n",
    "                        return True\n",
    "    return False\n",
    "\n",
    "def process_image(img):\n",
    "    global mtx, dist, line_prev, num_full_search\n",
    "    if mtx is None or dist is None:\n",
    "         mtx, dist = calibrate_camera(\"./camera_cal/calibration*.jpg\")\n",
    "    undist_img = cal_undistort(img, mtx, dist)\n",
    "    s_thresh = (175, 250)\n",
    "    sx_thresh = (30, 150)\n",
    "    binary_threshold = binary_thresholding(undist_img, s_thresh=s_thresh, sx_thresh=sx_thresh)\n",
    "    binary_warped, M = perspective_transform(binary_threshold)\n",
    "\n",
    "    left_lane_inds = None\n",
    "    right_lane_inds = None\n",
    "    out_img = None\n",
    "    plotSearchArea = True\n",
    "    line = None\n",
    "    if line_prev is None:\n",
    "        left_lane_inds, right_lane_inds, out_img = naive_find_lines(binary_warped)\n",
    "        plotSearchArea = False\n",
    "        line = process_fit(binary_warped, left_lane_inds, right_lane_inds)\n",
    "        num_full_search = num_full_search + 1\n",
    "    else:\n",
    "        left_lane_inds, right_lane_inds, out_img = smart_find_lines(binary_warped, \n",
    "            line_prev.left_fit, line_prev.right_fit)\n",
    "        line = process_fit(binary_warped, left_lane_inds, right_lane_inds)\n",
    "        # check for a good fit\n",
    "        if is_good_fit(line_prev, line) is False:\n",
    "            left_lane_inds, right_lane_inds, out_img = naive_find_lines(binary_warped)\n",
    "            plotSearchArea = False\n",
    "            line = process_fit(binary_warped, left_lane_inds, right_lane_inds)\n",
    "            num_full_search = num_full_search + 1\n",
    "\n",
    "    result = draw_lines(undist_img, binary_warped, line.yvals, line.left_fitx, line.right_fitx, np.linalg.inv(M))\n",
    "\n",
    "    annotated_result = annotate_result(result, line)\n",
    "\n",
    "    #display(img, 'Original', binary_threshold, 'Thresholded')\n",
    "    #display(binary_threshold, 'Thresholded', binary_warped, 'Transformed')\n",
    "    #display_poly_fit(binary_warped, left_lane_inds, right_lane_inds, out_img, plotSearchArea)\n",
    "    #plt.imshow(result)\n",
    "    #plt.show()\n",
    "\n",
    "    line_prev = line\n",
    "    return annotated_result\n",
    "\n",
    "def process_video():\n",
    "    global num_full_search\n",
    "    output = 'project_output.mp4'\n",
    "    clip1 = VideoFileClip(\"project_video.mp4\")\n",
    "    output_clip = clip1.fl_image(process_image)\n",
    "    output_clip.write_videofile(output, audio=False)\n",
    "    print(\"Num full searches\", num_full_search)\n",
    "\n",
    "process_video()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Discussion**\n",
    "\n",
    "There are several places where we have some hardcoded heuristics. They are:\n",
    "\n",
    "1. The warp perspective coordinates are heuristics tuned based on the project video and its camera perspective. If we need this to generalize then we need to an automated way to detect these coordinates based on camera angles.\n",
    "\n",
    "2. The binary threshoding coefficients once again were tuned for this video. While this does generalize some, it has limitations. We need a more generic solution if this needs to work on any terrain, any kind of lighting, rain, snow etc.\n",
    "\n",
    "3. My \"smart\" line detection only takes into previous frame. There is no reason why this cannot be a sliding window of last N frames and the poly fit coefficients can be averaged over last N frames. This might generalize better and would prevent lane from jumping too much on challenge videos."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
