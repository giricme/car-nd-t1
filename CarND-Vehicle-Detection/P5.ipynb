{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Vehicle Detection** \n",
    "***\n",
    "In this project, we write a software pipeline to identify vehicles in a video from a front-facing camera on a car. \n",
    "\n",
    "## **Overview**\n",
    "\n",
    "Here is just an over view of the pipeline. We provide details below.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from moviepy.editor import VideoFileClip\n",
    "import pickle\n",
    "import glob\n",
    "import time\n",
    "from skimage.feature import hog\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from scipy.ndimage.measurements import label\n",
    "from scipy.ndimage import generate_binary_structure\n",
    "from collections import deque\n",
    "from numpy import linalg as LA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Visualizing Data**\n",
    "\n",
    "First step in any analysis is to understand the data set. Here we will visualize a few of of the car and non-car samples. The images are 64x64 pixels in dimension.\n",
    "\n",
    "<p align=\"center\">\n",
    " <img src=\"./output_images/car_no_car_visualization.png\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "car1 = cv2.imread(\"./test_data/vehicles_smallset/cars1/1.jpeg\")\n",
    "car2 = cv2.imread(\"./test_data/vehicles/GTI_Far/image0000.png\")\n",
    "noncar1 = cv2.imread(\"./test_data/non-vehicles_smallset/notcars1/extra00.jpeg\")\n",
    "noncar2 = cv2.imread(\"./test_data/non-vehicles/Extras/extra27.png\")\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(221)  #top left\n",
    "ax1.imshow(car1)\n",
    "ax2 = fig.add_subplot(222)  #top right\n",
    "ax2.imshow(car2)\n",
    "ax3 = fig.add_subplot(223)   #bottom left\n",
    "ax3.imshow(noncar1)\n",
    "ax4 = fig.add_subplot(224) \n",
    "ax4.imshow(noncar2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Feature Extraction**\n",
    "\n",
    "Lectures recommend that we use combination of features. \n",
    "\n",
    "### **Spatial Binning**\n",
    "\n",
    "Using features directly from image pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function to compute binned color features  \n",
    "def bin_spatial(img, size=(32, 32)):\n",
    "    # Use cv2.resize().ravel() to create the feature vector\n",
    "    features = cv2.resize(img, size).ravel() \n",
    "    # Return the feature vector\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Color Histograms**\n",
    "First we start with color space exploration. Just like using raw pixel intensities as features and we look at histograms of pixel intensity (color histograms) as features. We explore the color channel histograms for a sample car and non-car image for:\n",
    "\n",
    "1. RGB color space (All three channels, RGB, give quite a distinctive signature between car and non-car histograms)\n",
    "2. HSV color space (All three channels, HSV provide quite distinctive distributions between car and non-car image)\n",
    "3. YCrCb color space (Y channel is distinctive between car and non-car images but Cr and Cb are rather close)\n",
    "\n",
    "<p align=\"center\">\n",
    " <img src=\"./output_images/rgb_color_histogram.png\">\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    " <img src=\"./output_images/hsv_color_histogram.png\">\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    " <img src=\"./output_images/ycrcb_color_histogram.png\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define a function to compute color histogram features  \n",
    "def color_hist(image, nbins=32, bins_range=(0, 256)):\n",
    "    # Compute the histogram of the channels separately\n",
    "    # Take histograms of each channel\n",
    "    c1, c2, c3 = image[:,:,0], image[:,:,1], image[:,:,2]\n",
    "    c1hist = np.histogram(c1, bins=nbins, range=bins_range)\n",
    "    c2hist = np.histogram(c2, bins=nbins, range=bins_range)\n",
    "    c3hist = np.histogram(c3, bins=nbins, range=bins_range)\n",
    "    # Generating bin centers\n",
    "    bin_edges = c1hist[1]\n",
    "    bin_centers = (bin_edges[1:]  + bin_edges[0:len(bin_edges)-1])/2\n",
    "    # Concatenate the histograms into a single feature vector\n",
    "    hist_features = np.concatenate((c1hist[0], c2hist[0], c3hist[0]))\n",
    "    # Return the individual histograms, bin_centers and feature vector\n",
    "    return c1hist, c2hist, c3hist, bin_centers, hist_features\n",
    "\n",
    "def display_color_histograms(c1, c2, c3, c1_label, c2_label, c3_label):\n",
    "    plt.subplot(131)\n",
    "    plt.bar(bincen, c1[0])\n",
    "    plt.xlim(0, 256)\n",
    "    plt.title(c1_label)\n",
    "    plt.subplot(132)\n",
    "    plt.bar(bincen, c2[0])\n",
    "    plt.xlim(0, 256)\n",
    "    plt.title(c2_label)\n",
    "    plt.subplot(133)\n",
    "    plt.bar(bincen, c3[0])\n",
    "    plt.xlim(0, 256)\n",
    "    plt.title(c3_label)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "bins = 32\n",
    "bins_range = (0, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **RGB**\n",
    "\n",
    "All three channels, RGB, give quite a distinctive signature between car and non-car histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "car_bgr = cv2.imread(\"./test_data/vehicles/GTI_Far/image0000.png\")\n",
    "noncar_bgr = cv2.imread(\"./test_data/non-vehicles/Extras/extra27.png\")\n",
    "\n",
    "car_b, car_g, car_r, bincen, feature_vec = color_hist(car_bgr, nbins=bins, bins_range=bins_range)\n",
    "noncar_r, noncar_g, noncar_b, bincen, feature_vec = color_hist(noncar_bgr, nbins=bins, bins_range=bins_range)\n",
    "\n",
    "display_color_histograms(car_r, car_g, car_b, 'Car R', 'Car G', 'Car B')\n",
    "display_color_histograms(noncar_r, noncar_g, noncar_b, 'Non-Car R', 'Non-Car G', 'Non-Car B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **HSV**\n",
    "\n",
    "All three channels, HSV provide quite distinctive distributions between car and non-car image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "car_hsv = cv2.cvtColor(car_bgr, cv2.COLOR_BGR2HSV)\n",
    "noncar_hsv = cv2.cvtColor(noncar_bgr, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "car_h, car_s, car_v, bincen, feature_vec = color_hist(car_hsv, nbins=bins, bins_range=bins_range)\n",
    "noncar_h, noncar_s, noncar_v, bincen, feature_vec = color_hist(noncar_hsv, nbins=bins, bins_range=bins_range)\n",
    "\n",
    "display_color_histograms(car_h, car_s, car_v, 'Car H', 'Car S', 'Car V')\n",
    "display_color_histograms(noncar_h, noncar_s, noncar_v, 'Non-Car H', 'Non-Car S', 'Non-Car V')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **YCrCb**\n",
    "\n",
    "Y channel is distinctive between car and non-car images but Cr and Cb are rather close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "car_ycrcb = cv2.cvtColor(car_bgr, cv2.COLOR_BGR2YCrCb)\n",
    "noncar_ycrcb = cv2.cvtColor(noncar_bgr, cv2.COLOR_BGR2YCrCb)\n",
    "\n",
    "car_y, car_cr, car_cb, bincen, feature_vec = color_hist(car_ycrcb, nbins=bins, bins_range=bins_range)\n",
    "noncar_y, noncar_cr, noncar_cb, bincen, feature_vec = color_hist(noncar_ycrcb, nbins=bins, bins_range=bins_range)\n",
    "\n",
    "display_color_histograms(car_y, car_cr, car_cb, 'Car Y', 'Car Cr', 'Car Cb')\n",
    "display_color_histograms(noncar_y, noncar_cr, noncar_cb, 'Non-Car Y', 'Non-Car Cr', 'Non-Car Cb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Histogram of oriented gradients (HOG)**\n",
    "\n",
    "One can tune the following parameters while using HOG:\n",
    "\n",
    "1. Number of Orientations - The number of orientations is specified as an integer, and represents the number of orientation bins that the gradient information will be split up into in the histogram. Typical values are between 6 and 12 bins.\n",
    "2. Pixels per Cell - The pixels_per_cell parameter specifies the cell size over which each gradient histogram is computed.\n",
    "3. Cells per Block - The cells_per_block parameter is also passed as a 2-tuple, and specifies the local area over which the histogram counts in a given cell will be normalized. Block normalization is not necessarily required, but generally leads to a more robust feature set.\n",
    "\n",
    "We tried:\n",
    "\n",
    "1. Orientations = 8, Pixel per cell = (8,8), cells per block = 2\n",
    "2. Orientations = 9, Pixel per cell = (8,8), cells per block = 2\n",
    "3. Orientations = 8, Pixel per cell = (16,16), cells per block = 2\n",
    "4. Orientations = 9, Pixel per cell = (16,16), cells per block = 2\n",
    "\n",
    "It turns out (1) and (2) are close. We go with (1) since it has fewer number of features. \n",
    "\n",
    "We also experimented with HOG features on color spaces. Following were tried:\n",
    "\n",
    "1. HOG over gray scale (BGR-2-GRAY)\n",
    "2. HOG over individual R, G, B channels.\n",
    "3. HOG over individual H, S, V channels. In HSV, V channel is the equivalent of gray.\n",
    "4. HOG over individual Y, Cr, Cb channels. In YCrCb, Y channle is the equivalent of gray.\n",
    "\n",
    "The results are somewhat inconclusive. It looks like all three color spaces, provide the ability to distinguish between car and non car images using HOG features.\n",
    "\n",
    "HOG RGB \n",
    "<p align=\"center\">\n",
    " <img src=\"./output_images/hog_rgb_gray.png\">\n",
    "</p>\n",
    "\n",
    "HOG R\n",
    "<p align=\"center\">\n",
    " <img src=\"./output_images/hog_r.png\">\n",
    "</p>\n",
    "\n",
    "HOG G\n",
    "<p align=\"center\">\n",
    " <img src=\"./output_images/hog_g.png\">\n",
    "</p>\n",
    "\n",
    "HOG B\n",
    "<p align=\"center\">\n",
    " <img src=\"./output_images/hog_b.png\">\n",
    "</p>\n",
    "\n",
    "HOG H\n",
    "<p align=\"center\">\n",
    " <img src=\"./output_images/hog_h.png\">\n",
    "</p>\n",
    "\n",
    "HOG S\n",
    "<p align=\"center\">\n",
    " <img src=\"./output_images/hog_s.png\">\n",
    "</p>\n",
    "\n",
    "HOG V\n",
    "<p align=\"center\">\n",
    " <img src=\"./output_images/hog_v.png\">\n",
    "</p>\n",
    "\n",
    "HOG Y\n",
    "<p align=\"center\">\n",
    " <img src=\"./output_images/hog_y.png\">\n",
    "</p>\n",
    "\n",
    "HOG Cr\n",
    "<p align=\"center\">\n",
    " <img src=\"./output_images/hog_cr.png\">\n",
    "</p>\n",
    "\n",
    "HOG Cb\n",
    "<p align=\"center\">\n",
    " <img src=\"./output_images/hog_cb.png\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define a function to return HOG features and visualization\n",
    "def get_hog_features(img, orient, pix_per_cell, cell_per_block, \n",
    "                        vis=False, feature_vec=True):\n",
    "    # Call with two outputs if vis==True\n",
    "    if vis == True:\n",
    "        features, hog_image = hog(img, orientations=orient, \n",
    "                                  pixels_per_cell=(pix_per_cell, pix_per_cell),\n",
    "                                  cells_per_block=(cell_per_block, cell_per_block), \n",
    "                                  transform_sqrt=True, \n",
    "                                  visualise=vis, feature_vector=feature_vec)\n",
    "        return features, hog_image\n",
    "    # Otherwise call with one output\n",
    "    else:      \n",
    "        features = hog(img, orientations=orient, \n",
    "                       pixels_per_cell=(pix_per_cell, pix_per_cell),\n",
    "                       cells_per_block=(cell_per_block, cell_per_block), \n",
    "                       transform_sqrt=True, \n",
    "                       visualise=vis, feature_vector=feature_vec)\n",
    "        return features\n",
    "\n",
    "def hog_experiment(car_gray, noncar_gray):\n",
    "    # Define HOG parameters\n",
    "    orient = 8\n",
    "    pix_per_cell = 8\n",
    "    cell_per_block = 2\n",
    "    # Call our function with vis=True to see an image output\n",
    "    features, car_hog_image_882 = get_hog_features(car_gray, orient, pix_per_cell, cell_per_block, \n",
    "                                               vis=True, feature_vec=False)\n",
    "    features, noncar_hog_image_882 = get_hog_features(noncar_gray, orient, pix_per_cell, cell_per_block, \n",
    "                                               vis=True, feature_vec=False)\n",
    "\n",
    "    # Define HOG parameters\n",
    "    orient = 8\n",
    "    pix_per_cell = 16\n",
    "    cell_per_block = 2\n",
    "    # Call our function with vis=True to see an image output\n",
    "    features, car_hog_image_8162 = get_hog_features(car_gray, orient, pix_per_cell, cell_per_block, \n",
    "                                               vis=True, feature_vec=False)\n",
    "    features, noncar_hog_image_8162 = get_hog_features(noncar_gray, orient, pix_per_cell, cell_per_block, \n",
    "                                               vis=True, feature_vec=False)\n",
    "\n",
    "    # Define HOG parameters\n",
    "    orient = 9\n",
    "    pix_per_cell = 8\n",
    "    cell_per_block = 2\n",
    "    # Call our function with vis=True to see an image output\n",
    "    features, car_hog_image_982 = get_hog_features(car_gray, orient, pix_per_cell, cell_per_block, \n",
    "                                               vis=True, feature_vec=False)\n",
    "    features, noncar_hog_image_982 = get_hog_features(noncar_gray, orient, pix_per_cell, cell_per_block, \n",
    "                                               vis=True, feature_vec=False)\n",
    "\n",
    "    # Define HOG parameters\n",
    "    orient = 9\n",
    "    pix_per_cell = 16\n",
    "    cell_per_block = 2\n",
    "    # Call our function with vis=True to see an image output\n",
    "    features, car_hog_image_9162 = get_hog_features(car_gray, orient, pix_per_cell, cell_per_block, \n",
    "                                               vis=True, feature_vec=False)\n",
    "    features, noncar_hog_image_9162 = get_hog_features(noncar_gray, orient, pix_per_cell, cell_per_block, \n",
    "                                               vis=True, feature_vec=False)\n",
    "\n",
    "    return car_hog_image_882, noncar_hog_image_882, car_hog_image_8162, noncar_hog_image_8162, car_hog_image_982, noncar_hog_image_982, car_hog_image_9162, noncar_hog_image_9162\n",
    "\n",
    "def plot_hog(car_gray, noncar_gray,\n",
    "             car_hog_image_882, noncar_hog_image_882, \n",
    "             car_hog_image_8162, noncar_hog_image_8162, \n",
    "             car_hog_image_982, noncar_hog_image_982, \n",
    "             car_hog_image_9162, noncar_hog_image_9162):\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(251)  \n",
    "    ax1.axis('off')\n",
    "    ax1.set_title('Orig')\n",
    "    ax1.imshow(car_gray)\n",
    "    ax2 = fig.add_subplot(252)  \n",
    "    ax2.axis('off')\n",
    "    ax2.set_title('8-8-2')\n",
    "    ax2.imshow(car_hog_image_882, cmap='gray')\n",
    "    ax3 = fig.add_subplot(253)\n",
    "    ax3.axis('off')\n",
    "    ax3.set_title('8-16-2')\n",
    "    ax3.imshow(car_hog_image_8162, cmap='gray')\n",
    "    ax4 = fig.add_subplot(254) \n",
    "    ax4.axis('off')\n",
    "    ax4.set_title('9-8-2')\n",
    "    ax4.imshow(car_hog_image_982, cmap='gray')\n",
    "    ax5 = fig.add_subplot(255) \n",
    "    ax5.axis('off')\n",
    "    ax5.set_title('9-16-2')\n",
    "    ax5.imshow(car_hog_image_9162, cmap='gray')\n",
    "    ax6 = fig.add_subplot(256)  \n",
    "    ax6.axis('off')\n",
    "    ax6.set_title('Orig')\n",
    "    ax6.imshow(noncar_gray)\n",
    "    ax7 = fig.add_subplot(257)  \n",
    "    ax7.axis('off')\n",
    "    ax7.set_title('8-8-2')\n",
    "    ax7.imshow(noncar_hog_image_882, cmap='gray')\n",
    "    ax8 = fig.add_subplot(258)\n",
    "    ax8.axis('off')\n",
    "    ax8.set_title('8-16-2')\n",
    "    ax8.imshow(noncar_hog_image_8162, cmap='gray')\n",
    "    ax9 = fig.add_subplot(259) \n",
    "    ax9.axis('off')\n",
    "    ax9.set_title('9-8-2')\n",
    "    ax9.imshow(noncar_hog_image_982, cmap='gray')\n",
    "    ax10 = fig.add_subplot(2,5,10) \n",
    "    ax10.axis('off')\n",
    "    ax10.set_title('9-16-2')\n",
    "    ax10.imshow(noncar_hog_image_9162, cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "print(\"RGB-Gray HOG Features\")\n",
    "car_gray = cv2.cvtColor(car_bgr, cv2.COLOR_BGR2GRAY)\n",
    "noncar_gray = cv2.cvtColor(noncar_bgr, cv2.COLOR_BGR2GRAY)\n",
    "car_hog_image_882, noncar_hog_image_882, car_hog_image_8162, noncar_hog_image_8162, car_hog_image_982, noncar_hog_image_982, car_hog_image_9162, noncar_hog_image_9162 = hog_experiment(car_gray, noncar_gray)\n",
    "plot_hog(car_gray, noncar_gray, car_hog_image_882, noncar_hog_image_882, \n",
    "             car_hog_image_8162, noncar_hog_image_8162, \n",
    "             car_hog_image_982, noncar_hog_image_982, \n",
    "             car_hog_image_9162, noncar_hog_image_9162)\n",
    "\n",
    "car_b, car_g, car_r = cv2.split(car_bgr)\n",
    "noncar_b, noncar_g, noncar_r = cv2.split(noncar_bgr)\n",
    "\n",
    "print(\"R Channel HOG Features\")\n",
    "car_hog_image_882, noncar_hog_image_882, car_hog_image_8162, noncar_hog_image_8162, car_hog_image_982, noncar_hog_image_982, car_hog_image_9162, noncar_hog_image_9162 = hog_experiment(car_r, noncar_r)\n",
    "plot_hog(car_r, noncar_r, car_hog_image_882, noncar_hog_image_882, \n",
    "             car_hog_image_8162, noncar_hog_image_8162, \n",
    "             car_hog_image_982, noncar_hog_image_982, \n",
    "             car_hog_image_9162, noncar_hog_image_9162)\n",
    "\n",
    "print(\"G Channel HOG Features\")\n",
    "car_hog_image_882, noncar_hog_image_882, car_hog_image_8162, noncar_hog_image_8162, car_hog_image_982, noncar_hog_image_982, car_hog_image_9162, noncar_hog_image_9162 = hog_experiment(car_g, noncar_g)\n",
    "plot_hog(car_g, noncar_g, car_hog_image_882, noncar_hog_image_882, \n",
    "             car_hog_image_8162, noncar_hog_image_8162, \n",
    "             car_hog_image_982, noncar_hog_image_982, \n",
    "             car_hog_image_9162, noncar_hog_image_9162)\n",
    "\n",
    "print(\"B Channel HOG Features\")\n",
    "car_hog_image_882, noncar_hog_image_882, car_hog_image_8162, noncar_hog_image_8162, car_hog_image_982, noncar_hog_image_982, car_hog_image_9162, noncar_hog_image_9162 = hog_experiment(car_b, noncar_b)\n",
    "plot_hog(car_b, noncar_b, car_hog_image_882, noncar_hog_image_882, \n",
    "             car_hog_image_8162, noncar_hog_image_8162, \n",
    "             car_hog_image_982, noncar_hog_image_982, \n",
    "             car_hog_image_9162, noncar_hog_image_9162)\n",
    "\n",
    "car_h, car_s, car_v = cv2.split(car_hsv)\n",
    "noncar_h, noncar_s, noncar_v = cv2.split(noncar_hsv)\n",
    "\n",
    "print(\"H Channel HOG Features\")\n",
    "car_hog_image_882, noncar_hog_image_882, car_hog_image_8162, noncar_hog_image_8162, car_hog_image_982, noncar_hog_image_982, car_hog_image_9162, noncar_hog_image_9162 = hog_experiment(car_h, noncar_h)\n",
    "plot_hog(car_hsv, noncar_hsv, car_hog_image_882, noncar_hog_image_882, \n",
    "             car_hog_image_8162, noncar_hog_image_8162, \n",
    "             car_hog_image_982, noncar_hog_image_982, \n",
    "             car_hog_image_9162, noncar_hog_image_9162)\n",
    "\n",
    "print(\"S Channel HOG Features\")\n",
    "car_hog_image_882, noncar_hog_image_882, car_hog_image_8162, noncar_hog_image_8162, car_hog_image_982, noncar_hog_image_982, car_hog_image_9162, noncar_hog_image_9162 = hog_experiment(car_s, noncar_s)\n",
    "plot_hog(car_hsv, noncar_hsv, car_hog_image_882, noncar_hog_image_882, \n",
    "             car_hog_image_8162, noncar_hog_image_8162, \n",
    "             car_hog_image_982, noncar_hog_image_982, \n",
    "             car_hog_image_9162, noncar_hog_image_9162)\n",
    "\n",
    "print(\"V Channel HOG Features\")\n",
    "car_hog_image_882, noncar_hog_image_882, car_hog_image_8162, noncar_hog_image_8162, car_hog_image_982, noncar_hog_image_982, car_hog_image_9162, noncar_hog_image_9162 = hog_experiment(car_v, noncar_v)\n",
    "plot_hog(car_hsv, noncar_hsv, car_hog_image_882, noncar_hog_image_882, \n",
    "             car_hog_image_8162, noncar_hog_image_8162, \n",
    "             car_hog_image_982, noncar_hog_image_982, \n",
    "             car_hog_image_9162, noncar_hog_image_9162)\n",
    "\n",
    "car_y, car_cr, car_cb = cv2.split(car_ycrcb)\n",
    "noncar_y, noncar_cr, noncar_cb = cv2.split(noncar_ycrcb)\n",
    "\n",
    "print(\"Y Channel HOG Features\")\n",
    "car_hog_image_882, noncar_hog_image_882, car_hog_image_8162, noncar_hog_image_8162, car_hog_image_982, noncar_hog_image_982, car_hog_image_9162, noncar_hog_image_9162 = hog_experiment(car_y, noncar_y)\n",
    "plot_hog(car_ycrcb, noncar_ycrcb, car_hog_image_882, noncar_hog_image_882, \n",
    "             car_hog_image_8162, noncar_hog_image_8162, \n",
    "             car_hog_image_982, noncar_hog_image_982, \n",
    "             car_hog_image_9162, noncar_hog_image_9162)\n",
    "\n",
    "print(\"Cr Channel HOG Features\")\n",
    "car_hog_image_882, noncar_hog_image_882, car_hog_image_8162, noncar_hog_image_8162, car_hog_image_982, noncar_hog_image_982, car_hog_image_9162, noncar_hog_image_9162 = hog_experiment(car_cr, noncar_cr)\n",
    "plot_hog(car_ycrcb, noncar_ycrcb, car_hog_image_882, noncar_hog_image_882, \n",
    "             car_hog_image_8162, noncar_hog_image_8162, \n",
    "             car_hog_image_982, noncar_hog_image_982, \n",
    "             car_hog_image_9162, noncar_hog_image_9162)\n",
    "\n",
    "print(\"Cb Channel HOG Features\")\n",
    "car_hog_image_882, noncar_hog_image_882, car_hog_image_8162, noncar_hog_image_8162, car_hog_image_982, noncar_hog_image_982, car_hog_image_9162, noncar_hog_image_9162 = hog_experiment(car_cb, noncar_cb)\n",
    "plot_hog(car_ycrcb, noncar_ycrcb, car_hog_image_882, noncar_hog_image_882, \n",
    "             car_hog_image_8162, noncar_hog_image_8162, \n",
    "             car_hog_image_982, noncar_hog_image_982, \n",
    "             car_hog_image_9162, noncar_hog_image_9162)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Classifier**\n",
    "\n",
    "First we need to make sure all features are normalized. We use sklearn StandardScaler to do this. \n",
    "\n",
    "### **Model Selection**\n",
    "\n",
    "Next we try a few different feature combinations. \n",
    "\n",
    "1. RGB with HOG features with spatial and color histogram features.\n",
    "2. HSV with HOG features with spatial and color histogram features.\n",
    "3. YCrCb with HOG features with spatial and color histogram features.\n",
    "4. RGB with HOG features from ALL channels.\n",
    "5. HSV with HOG features from ALL channels.\n",
    "6. YCrCb with HOG features from ALL channels.\n",
    "\n",
    "As we did this experiment we hold HOG feature generation parameters fixed as follows:\n",
    "\n",
    "- Spatial size = (32,32)\n",
    "- Color histogram bins = 32\n",
    "- HOG orientations = 8\n",
    "- HOG Pixels per cell = 8\n",
    "- HOG Cells per block = 2\n",
    "- HOG channels = ALL\n",
    "\n",
    "Results:\n",
    "\n",
    "```\n",
    "    11.68 Seconds to extract HOG features...\n",
    "    0.84 Seconds to train SVC...\n",
    "    RGB Test Accuracy for SVC with spatial and color hist =  0.9785\n",
    "    10.11 Seconds to extract HOG features...\n",
    "    0.49 Seconds to train SVC...\n",
    "    HSV Test Accuracy for SVC with spatial and color hist =  0.9892\n",
    "    10.57 Seconds to extract HOG features...\n",
    "    0.47 Seconds to train SVC...\n",
    "    YCrCb Test Accuracy for SVC with spatial and color hist =  0.9978\n",
    "    8.56 Seconds to extract HOG features...\n",
    "    0.41 Seconds to train SVC...\n",
    "    RGB Test Accuracy for SVC without spatial and color hist =  0.9462\n",
    "    9.04 Seconds to extract HOG features...\n",
    "    0.25 Seconds to train SVC...\n",
    "    HSV Test Accuracy for SVC without spatial and color hist =  0.9849\n",
    "    8.4 Seconds to extract HOG features...\n",
    "    0.25 Seconds to train SVC...\n",
    "    YCrCb Test Accuracy for SVC without spatial and color hist =  0.9935\n",
    "```\n",
    "\n",
    "We find the **YCrCb color space with HOG, spatial and color histogram features** provides a test accuracy of **99.7%** We use this moving forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function to extract features from a single image window\n",
    "# This function is very similar to extract_features()\n",
    "# just for a single image rather than list of images\n",
    "def single_img_features(img, color_space='YCrCb', spatial_size=(32, 32),\n",
    "                        hist_bins=32, orient=8, \n",
    "                        pix_per_cell=8, cell_per_block=2, hog_channel=\"ALL\",\n",
    "                        spatial_feat=True, hist_feat=True, hog_feat=True):   \n",
    "    #1) Define an empty list to receive features\n",
    "    img_features = []\n",
    "    #2) Apply color conversion if other than 'RGB'\n",
    "    if color_space != 'RGB':\n",
    "        if color_space == 'HSV':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
    "        elif color_space == 'LUV':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2LUV)\n",
    "        elif color_space == 'HLS':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)\n",
    "        elif color_space == 'YUV':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2YUV)\n",
    "        elif color_space == 'YCrCb':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2YCrCb)\n",
    "    else: \n",
    "        feature_image = np.copy(img) \n",
    "    #3) Normalize image so that values are between 0 and 1\n",
    "    image_min = np.min(feature_image)\n",
    "    image_max = np.max(feature_image)\n",
    "    feature_image = (feature_image - image_min) / (image_max - image_min)\n",
    "    #4) Compute spatial features if flag is set\n",
    "    if spatial_feat == True:\n",
    "        spatial_features = bin_spatial(feature_image, size=spatial_size)\n",
    "        #5) Append features to list\n",
    "        img_features.append(spatial_features)\n",
    "    #6) Compute histogram features if flag is set\n",
    "    if hist_feat == True:\n",
    "        c1hist, c2hist, c3hist, bin_centers, hist_features = color_hist(feature_image, nbins=hist_bins)\n",
    "        #7) Append features to list\n",
    "        img_features.append(hist_features)\n",
    "    #8) Compute HOG features if flag is set\n",
    "    if hog_feat == True:\n",
    "        if hog_channel == 'ALL':\n",
    "            hog_features = []\n",
    "            for channel in range(feature_image.shape[2]):\n",
    "                hog_features.extend(get_hog_features(feature_image[:,:,channel], \n",
    "                                    orient, pix_per_cell, cell_per_block, \n",
    "                                    vis=False, feature_vec=True))      \n",
    "        else:\n",
    "            hog_features = get_hog_features(feature_image[:,:,hog_channel], orient, \n",
    "                        pix_per_cell, cell_per_block, vis=False, feature_vec=True)\n",
    "        #9) Append features to list\n",
    "        img_features.append(hog_features)\n",
    "\n",
    "    #10) Return concatenated array of features\n",
    "    return np.concatenate(img_features)\n",
    "\n",
    "def extract_features(images, cspace='YCrCb', spatial_feat=True, hist_feat=True):\n",
    "    # Create a list to append feature vectors to\n",
    "    features = []\n",
    "    # Iterate through the list of images\n",
    "    for file in images:   \n",
    "        # Read in each one by one\n",
    "        image = cv2.imread(file)\n",
    "        img_features = single_img_features(image, color_space=cspace, \n",
    "                                           spatial_feat=spatial_feat, \n",
    "                                           hist_feat=hist_feat)\n",
    "        features.append(img_features)\n",
    "    return features\n",
    "\n",
    "def split(X, y):\n",
    "    # Split up data into randomized training and test sets\n",
    "    rand_state = np.random.randint(0, 100)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=rand_state)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def classifier_experiment(cars, notcars, colorspace, spatial_feat=True, hist_feat=True):\n",
    "    t=time.time()\n",
    "    car_features = extract_features(cars, cspace=colorspace, spatial_feat=spatial_feat, hist_feat=hist_feat)\n",
    "    notcar_features = extract_features(notcars, cspace=colorspace, spatial_feat=spatial_feat, hist_feat=hist_feat)\n",
    "    t2 = time.time()\n",
    "    print(round(t2-t, 2), 'Seconds to extract HOG features...')\n",
    "    # Create an array stack of feature vectors\n",
    "    X = np.vstack((car_features, notcar_features)).astype(np.float64)                        \n",
    "    # Fit a per-column scaler\n",
    "    X_scaler = StandardScaler().fit(X)\n",
    "    # Apply the scaler to X\n",
    "    scaled_X = X_scaler.transform(X)\n",
    "    # Define the labels vector\n",
    "    y = np.hstack((np.ones(len(car_features)), np.zeros(len(notcar_features))))\n",
    "    X_train, X_test, y_train, y_test = split(scaled_X, y)\n",
    "    \n",
    "    # Use a linear SVC \n",
    "    svc = LinearSVC()\n",
    "    # Check the training time for the SVC\n",
    "    t=time.time()\n",
    "    svc.fit(X_train, y_train)\n",
    "    t2 = time.time()\n",
    "    print(round(t2-t, 2), 'Seconds to train SVC...')\n",
    "    test_accuracy = svc.score(X_test, y_test)\n",
    "    # Check the score of the SVC\n",
    "    # print(colorspace, 'Test Accuracy for SVC = ', round(test_accuracy, 4))\n",
    "    return test_accuracy, svc\n",
    "\n",
    "cars = glob.iglob('./test_data/vehicles_smallset/**/*.jpeg', recursive=True)\n",
    "notcars = glob.iglob('./test_data/non-vehicles_smallset/**/*.jpeg', recursive=True)\n",
    "colorspace = 'RGB' \n",
    "test_accuracy, svc = classifier_experiment(cars, notcars, colorspace, spatial_feat=True, hist_feat=True)\n",
    "print(colorspace, 'Test Accuracy for SVC with spatial and color hist = ', round(test_accuracy, 4))\n",
    "\n",
    "cars = glob.iglob('./test_data/vehicles_smallset/**/*.jpeg', recursive=True)\n",
    "notcars = glob.iglob('./test_data/non-vehicles_smallset/**/*.jpeg', recursive=True)\n",
    "colorspace = 'HSV' \n",
    "test_accuracy, svc = classifier_experiment(cars, notcars, colorspace, spatial_feat=True, hist_feat=True)\n",
    "print(colorspace, 'Test Accuracy for SVC with spatial and color hist = ', round(test_accuracy, 4))\n",
    "\n",
    "cars = glob.iglob('./test_data/vehicles_smallset/**/*.jpeg', recursive=True)\n",
    "notcars = glob.iglob('./test_data/non-vehicles_smallset/**/*.jpeg', recursive=True)\n",
    "colorspace = 'YCrCb' \n",
    "test_accuracy, svc = classifier_experiment(cars, notcars, colorspace, spatial_feat=True, hist_feat=True)\n",
    "print(colorspace, 'Test Accuracy for SVC with spatial and color hist = ', round(test_accuracy, 4))\n",
    "\n",
    "cars = glob.iglob('./test_data/vehicles_smallset/**/*.jpeg', recursive=True)\n",
    "notcars = glob.iglob('./test_data/non-vehicles_smallset/**/*.jpeg', recursive=True)\n",
    "colorspace = 'RGB' \n",
    "test_accuracy, svc = classifier_experiment(cars, notcars, colorspace, spatial_feat=False, hist_feat=False)\n",
    "print(colorspace, 'Test Accuracy for SVC without spatial and color hist = ', round(test_accuracy, 4))\n",
    "\n",
    "cars = glob.iglob('./test_data/vehicles_smallset/**/*.jpeg', recursive=True)\n",
    "notcars = glob.iglob('./test_data/non-vehicles_smallset/**/*.jpeg', recursive=True)\n",
    "colorspace = 'HSV' \n",
    "test_accuracy, svc = classifier_experiment(cars, notcars, colorspace, spatial_feat=False, hist_feat=False)\n",
    "print(colorspace, 'Test Accuracy for SVC without spatial and color hist = ', round(test_accuracy, 4))\n",
    "\n",
    "cars = glob.iglob('./test_data/vehicles_smallset/**/*.jpeg', recursive=True)\n",
    "notcars = glob.iglob('./test_data/non-vehicles_smallset/**/*.jpeg', recursive=True)\n",
    "colorspace = 'YCrCb' \n",
    "test_accuracy, svc = classifier_experiment(cars, notcars, colorspace, spatial_feat=False, hist_feat=False)\n",
    "print(colorspace, 'Test Accuracy for SVC without spatial and color hist = ', round(test_accuracy, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model Generation**\n",
    "\n",
    "Now that we have selected the optimal model and feature combinations based on the \"smallset\" inputs we use these to train a LinearSVC for a max iteration of 20000 and save those features, feature scaler and SVC model. We get a test accuracy of *99%*.\n",
    "\n",
    "#### **Hard Negative Mining**\n",
    "\n",
    "Based on the false positives detected in the test images and video, we took some of those samples and added them to the `noncars` training data. This reduced some of the false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_and_save_features(cars, notcars):\n",
    "    # We have nailed down all the parameters. So run with defaults.\n",
    "    t=time.time()\n",
    "    car_features = extract_features(cars)\n",
    "    notcar_features = extract_features(notcars)\n",
    "    t2 = time.time()\n",
    "    print(round(t2-t, 2), 'Seconds to extract features...')\n",
    "    # Create an array stack of feature vectors\n",
    "    X = np.vstack((car_features, notcar_features)).astype(np.float64)                        \n",
    "    # Fit a per-column scaler\n",
    "    X_scaler = StandardScaler().fit(X)\n",
    "    # Apply the scaler to X\n",
    "    scaled_X = X_scaler.transform(X)\n",
    "    # Define the labels vector\n",
    "    y = np.hstack((np.ones(len(car_features)), np.zeros(len(notcar_features))))\n",
    "    X_train, X_test, y_train, y_test = split(scaled_X, y)\n",
    "    print('Saving data to pickle file...')\n",
    "    try:\n",
    "        with open('features.pickle', 'wb') as pfile:\n",
    "            pickle.dump(\n",
    "                {\n",
    "                    'X_train': X_train,\n",
    "                    'y_train': y_train,\n",
    "                    'X_test': X_test,\n",
    "                    'y_test': y_test,\n",
    "                    'X_scaler': X_scaler\n",
    "                },\n",
    "                pfile, pickle.HIGHEST_PROTOCOL)\n",
    "    except Exception as e:\n",
    "        print('Unable to save data to', pickle_file, ':', e)\n",
    "        raise\n",
    "    return\n",
    "\n",
    "def load_features():\n",
    "    cars = glob.iglob('./test_data/vehicles/**/*.png', recursive=True)\n",
    "    notcars = glob.iglob('./test_data/non-vehicles/**/*.png', recursive=True)\n",
    "    # Check if a pickle file already exists\n",
    "    pickle_file = 'features.pickle'\n",
    "    if os.path.exists(pickle_file) == False:\n",
    "        generate_and_save_features(cars, notcars)\n",
    "    with open(pickle_file, 'rb') as f:\n",
    "        pickle_data = pickle.load(f)\n",
    "        X_train = pickle_data['X_train']\n",
    "        y_train = pickle_data['y_train']\n",
    "        X_test = pickle_data['X_test']\n",
    "        y_test = pickle_data['y_test']\n",
    "        X_scaler = pickle_data['X_scaler']\n",
    "        del pickle_data\n",
    "    print('Training and test data loaded')\n",
    "    print(\"Training feature size:\", X_train.shape)\n",
    "    print(\"Training labels size:\", y_train.shape)\n",
    "    print(\"Test feature size:\", X_test.shape)\n",
    "    print(\"Test labels size:\", y_test.shape)\n",
    "    return X_train, X_test, y_train, y_test, X_scaler\n",
    "\n",
    "def train_classifier():\n",
    "    pickle_file = 'svc.pickle'\n",
    "    if os.path.exists(pickle_file) == False:\n",
    "        X_train, X_test, y_train, y_test, X_scaler = load_features()\n",
    "        # Use a linear SVC \n",
    "        svc = LinearSVC(C=1000, max_iter=20000)\n",
    "        # Check the training time for the SVC\n",
    "        t=time.time()\n",
    "        svc.fit(X_train, y_train)\n",
    "        t2 = time.time()\n",
    "        print(round(t2-t, 2), 'Seconds to train SVC...')\n",
    "        test_accuracy = svc.score(X_test, y_test)\n",
    "        print('Test accuracy ', round(test_accuracy, 4))\n",
    "        # Save the model for easy access\n",
    "        pickle_file = 'svc.pickle'\n",
    "        print('Saving model to pickle file...')\n",
    "        try:\n",
    "            with open(pickle_file, 'wb') as pfile:\n",
    "                pickle.dump(\n",
    "                    {\n",
    "                        'svc': svc,\n",
    "                        'X_scaler': X_scaler\n",
    "                    },\n",
    "                    pfile, pickle.HIGHEST_PROTOCOL)\n",
    "        except Exception as e:\n",
    "            print('Unable to save data to', pickle_file, ':', e)\n",
    "            raise\n",
    "    else:\n",
    "        print('Model already exists, skipping training...')\n",
    "\n",
    "def load_model():\n",
    "    with open('svc.pickle', 'rb') as f:\n",
    "        pickle_data = pickle.load(f)\n",
    "        svc = pickle_data['svc']\n",
    "        X_scaler = pickle_data['X_scaler']\n",
    "        del pickle_data\n",
    "        return svc, X_scaler\n",
    "\n",
    "train_classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Vehicle Detection Pipeline**\n",
    "\n",
    "Now that we have a classifier to classify images as cars or non-cars, we start putting together a processing pipeline that detects cars in a larger video frame. The pipeline involves the following steps:\n",
    "\n",
    "#### *Sliding Windows* \n",
    "\n",
    "- We use a series of overlapping sliding windows across the lower part of the image (there is no reason to look for cars in the sky) to detect cars. \n",
    "- Each sliding window image is resized to 64x64 pixels (the input size expected by our SVC classifier).\n",
    "- We use window sizes with overlaps for ranges listed below:\n",
    "    - x_start_stop=[300, None], y_start_stop=[380, 480], xy_window=(96, 96), xy_overlap=(0.9, 0.9)\n",
    "    - x_start_stop=[300, None], y_start_stop=[400, 500], xy_window=(128, 128), xy_overlap=(0.8, 0.8)\n",
    "    - x_start_stop=[300, None], y_start_stop=[430, 550], xy_window=(144, 144), xy_overlap=(0.8, 0.8)\n",
    "    - x_start_stop=[300, None], y_start_stop=[460, 580], xy_window=(192, 192), xy_overlap=(0.8, 0.8)\n",
    "\n",
    "#### *Deduping & False Positives*\n",
    "\n",
    "- We run prediction on the windowed image and as long as the prediction returns with a certain confidence level (to weed out false positives as much as we can)\n",
    "- When we do overlapped sliding windows, several sliding windows would match a single vehicle (or parts of it). We want to de-dupe all of them and recognize a bounding box that covers all those overlapping vehicle windows as a single vehicle.\n",
    "- If the classifier is working well, then the \"hot\" parts of the map are where the vehicles are, and by imposing a threshold, we can reject areas affected by false positives. Once you have a thresholded heat-map, most straightforward solution is to use the label() function from scipy.ndimage.measurements to figure out how many vehicles we have in each frame and which pixels belong to which vehicles. Once we have done this, we combine all windows that have the same label by their max coordinates.\n",
    "- We also average the heat map over a history of past frames (typically 4-6) to weed out false positives. We take advantage of `scipy.ndimage.generate_binary_structure` to represent the history of heat maps as planes in third dimension. Basically we connect the same window across heat maps (as if it morphs) and pick a window based on it's presence in few consecutive frames. This means we have a slight delay before we start detecting the vehicle (see circa frames 150 - 170 in the test video) however once we detect it we lock on to it much more precisely.\n",
    "\n",
    "\n",
    "### *Testing on sample images*\n",
    "\n",
    "We run the pipeline on sample test images in `test_images` to see how it performs. Here is the output.\n",
    "\n",
    "<p align=\"center\">\n",
    " <img src=\"./output_images/test1_out.png\">\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    " <img src=\"./output_images/test2_out.png\">\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    " <img src=\"./output_images/test3_out.png\">\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    " <img src=\"./output_images/test4_out.png\">\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    " <img src=\"./output_images/test5_out.png\">\n",
    "</p>\n",
    "\n",
    "<p align=\"center\">\n",
    " <img src=\"./output_images/test6_out.png\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define a function that takes an image,\n",
    "# start and stop positions in both x and y, \n",
    "# window size (x and y dimensions),  \n",
    "# and overlap fraction (for both x and y)\n",
    "def slide_window(img, x_start_stop=[None, None], y_start_stop=[None, None], \n",
    "                    xy_window=(64, 64), xy_overlap=(0.5, 0.5)):\n",
    "    # If x and/or y start/stop positions not defined, set to image size\n",
    "    if x_start_stop[0] == None:\n",
    "        x_start_stop[0] = 0\n",
    "    if x_start_stop[1] == None:\n",
    "        x_start_stop[1] = img.shape[1]\n",
    "    if y_start_stop[0] == None:\n",
    "        y_start_stop[0] = 0\n",
    "    if y_start_stop[1] == None:\n",
    "        y_start_stop[1] = img.shape[0]\n",
    "    # Compute the span of the region to be searched    \n",
    "    xspan = x_start_stop[1] - x_start_stop[0]\n",
    "    yspan = y_start_stop[1] - y_start_stop[0]\n",
    "    # Compute the number of pixels per step in x/y\n",
    "    nx_pix_per_step = np.int(xy_window[0]*(1 - xy_overlap[0]))\n",
    "    ny_pix_per_step = np.int(xy_window[1]*(1 - xy_overlap[1]))\n",
    "    # Compute the number of windows in x/y\n",
    "    nx_windows = np.int(xspan/nx_pix_per_step) - 1\n",
    "    ny_windows = np.int(yspan/ny_pix_per_step) - 1\n",
    "    # Initialize a list to append window positions to\n",
    "    window_list = []\n",
    "    # Loop through finding x and y window positions\n",
    "    for ys in range(ny_windows):\n",
    "        for xs in range(nx_windows):\n",
    "            # Calculate window position\n",
    "            startx = xs*nx_pix_per_step + x_start_stop[0]\n",
    "            endx = startx + xy_window[0]\n",
    "            starty = ys*ny_pix_per_step + y_start_stop[0]\n",
    "            endy = starty + xy_window[1]\n",
    "            # Append window position to list\n",
    "            window_list.append(((startx, starty), (endx, endy)))\n",
    "    # Return the list of windows\n",
    "    return window_list\n",
    "\n",
    "# Define a function to draw bounding boxes\n",
    "def draw_boxes(img, bboxes, color=(0, 0, 255), thick=6):\n",
    "    # Make a copy of the image\n",
    "    imcopy = np.copy(img)\n",
    "    # Iterate through the bounding boxes\n",
    "    for bbox in bboxes:\n",
    "        # Draw a rectangle given bbox coordinates\n",
    "        cv2.rectangle(imcopy, bbox[0], bbox[1], color, thick)\n",
    "    # Return the image copy with boxes drawn\n",
    "    return imcopy\n",
    "\n",
    "def find_cars(img, windows, svc, feature_scaler, confidence):\n",
    "    result = []\n",
    "    for window in windows:\n",
    "        window_image = cv2.resize(img[window[0][1]:window[1][1], window[0][0]:window[1][0]], (64, 64))\n",
    "        img_features = single_img_features(window_image)\n",
    "        test_features = feature_scaler.transform(img_features)\n",
    "        prediction = svc.predict(test_features)\n",
    "        conf = svc.decision_function(test_features)\n",
    "        if prediction == 1 and conf > confidence:\n",
    "            result.append(window)\n",
    "    return result\n",
    "\n",
    "def add_heat(image, bbox_list):\n",
    "    heatmap = np.zeros_like(image[:,:,0]).astype(np.float)\n",
    "    # Iterate through list of bboxes\n",
    "    for box in bbox_list:\n",
    "        # Add += 1 for all pixels inside each bbox\n",
    "        # Assuming each \"box\" takes the form ((x1, y1), (x2, y2))\n",
    "        heatmap[box[0][1]:box[1][1], box[0][0]:box[1][0]] += 1\n",
    "    # Return updated heatmap\n",
    "    return heatmap\n",
    "\n",
    "def apply_threshold(heatmap, threshold):\n",
    "    # Zero out pixels below the threshold\n",
    "    heatmap[heatmap <= threshold] = 0\n",
    "    # Return thresholded map\n",
    "    return heatmap\n",
    "\n",
    "def draw_labeled_bboxes(img, labels):\n",
    "    # Iterate through all detected cars\n",
    "    for car_number in range(1, labels[1]+1):\n",
    "        # Find pixels with each car_number label value\n",
    "        nonzero = (labels[0] == car_number).nonzero()\n",
    "        # Identify x and y values of those pixels\n",
    "        nonzeroy = np.array(nonzero[0])\n",
    "        nonzerox = np.array(nonzero[1])\n",
    "        # Define a bounding box based on min/max x and y\n",
    "        bbox = ((np.min(nonzerox), np.min(nonzeroy)), (np.max(nonzerox), np.max(nonzeroy)))\n",
    "        # Draw the box on the image\n",
    "        cv2.rectangle(img, bbox[0], bbox[1], (0,0,255), 6)\n",
    "    # Return the image\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Pipeline:\n",
    "    def __init__(self, svc, scaler, confidence, heat_threshold, \n",
    "                 heatmap_history_count=0, start=0, end=-1, debug=False):\n",
    "        self.svc = svc\n",
    "        self.scaler = scaler\n",
    "        self.confidence = confidence\n",
    "        self.heat_threshold = heat_threshold\n",
    "        self.heatmap_history_count = heatmap_history_count\n",
    "        self.heatmap_history = deque([])\n",
    "        self.debug = []\n",
    "        self.img_count = 0\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.debug = debug\n",
    "\n",
    "    def check_operational_window(self):\n",
    "        retval = True\n",
    "        if self.start > 0:\n",
    "            if self.img_count < self.start:\n",
    "                retval = False      \n",
    "        if self.end != -1:\n",
    "            if self.img_count > self.end:\n",
    "                return False\n",
    "        return retval\n",
    "                \n",
    "    def apply_labels(self, heatmap):\n",
    "        labels = None\n",
    "        if self.heatmap_history_count > 0:\n",
    "            # standardise heatmap -\n",
    "            heatmap_std = heatmap.std(ddof=1)\n",
    "            if heatmap_std != 0.0:\n",
    "                heatmap = (heatmap-heatmap.mean())/heatmap_std\n",
    "            heatmap = apply_threshold(heatmap, np.max([heatmap.std(), 1]))\n",
    "            self.heatmap_history.append(heatmap)\n",
    "            if len(self.heatmap_history) > self.heatmap_history_count:\n",
    "                self.heatmap_history.popleft()\n",
    "            hh = np.array(self.heatmap_history)\n",
    "            # create a structure for connectivity\n",
    "            s = generate_binary_structure(hh.ndim, hh.ndim)\n",
    "            labels = label(hh, s)\n",
    "        else:\n",
    "            labels = label(heatmap)\n",
    "        return labels\n",
    "\n",
    "    def detect_bboxes(self, labels):\n",
    "        bboxes = []\n",
    "        # Iterate through all detected cars\n",
    "        for car_number in range(1, labels[1]+1):\n",
    "            # Find pixels with each car_number label value\n",
    "            nonzero = (labels[0] == car_number).nonzero()\n",
    "            # Identify x and y values of those pixels\n",
    "            nonzeroz = np.array(nonzero[0])\n",
    "            nonzeroy = np.array(nonzero[1])\n",
    "            nonzerox = np.array(nonzero[2])\n",
    "\n",
    "            nonzerox_min = np.min(nonzerox)\n",
    "            nonzerox_max = np.max(nonzerox)\n",
    "            nonzeroy_min = np.min(nonzeroy)\n",
    "            nonzeroy_max = np.max(nonzeroy)\n",
    "            nonzeroz_min = np.min(nonzeroz)\n",
    "            nonzeroz_max = np.max(nonzeroz)\n",
    "\n",
    "            # only add if they appear in contiguous planes\n",
    "            nplane_min_threshold = self.heatmap_history_count - 1\n",
    "            nplanes = nonzeroz_max-nonzeroz_min+1\n",
    "            if nplanes >= nplane_min_threshold:\n",
    "                bbox = ((nonzerox_min, nonzeroy_min),\n",
    "                        (nonzerox_max, nonzeroy_max))\n",
    "                bboxes.append(bbox)\n",
    "        return bboxes\n",
    "\n",
    "    def draw_detected_bboxes(self, img, bbox_list):\n",
    "        for bbox in bbox_list:\n",
    "            # Draw the box on the image\n",
    "            cv2.rectangle(img, bbox[0], bbox[1], (0,0,255), 6)\n",
    "        # Return the image\n",
    "        return img\n",
    "\n",
    "    def write_debug_image(self, img, count):\n",
    "        out_file = './debug/' + str(count) + '.png'\n",
    "        cv2.imwrite(out_file, img)\n",
    "          \n",
    "    def process_image(self, image):\n",
    "        self.img_count = self.img_count + 1\n",
    "        if self.check_operational_window() == False:\n",
    "            return np.zeros_like(image).astype(np.float)\n",
    "        \"\"\"\n",
    "        1. Pick windows in the image.\n",
    "        2. Check if those windows have cars.\n",
    "        3. Draw bounding boxes on those.\n",
    "        \"\"\"\n",
    "        windows = []\n",
    "        windows = slide_window(image, x_start_stop=[300, None], y_start_stop=[380, 480],\n",
    "                                           xy_window=(96, 96), xy_overlap=(0.9, 0.9))\n",
    "        windows += slide_window(image, x_start_stop=[300, None], y_start_stop=[400, 500],\n",
    "                                           xy_window=(128, 128), xy_overlap=(0.8, 0.8)) \n",
    "        windows += slide_window(image, x_start_stop=[300, None], y_start_stop=[430, 550],\n",
    "                                           xy_window=(144, 144), xy_overlap=(0.8, 0.8))  \n",
    "        windows += slide_window(image, x_start_stop=[300, None], y_start_stop=[460, 580],\n",
    "                                           xy_window=(192, 192), xy_overlap=(0.8, 0.8))\n",
    "        car_windows = find_cars(image, windows, self.svc, self.scaler, self.confidence)\n",
    "        # Add heat to each box in box list\n",
    "        heat = add_heat(image, car_windows)  \n",
    "        # Apply threshold to help remove false positives\n",
    "        heat = apply_threshold(heat, self.heat_threshold)\n",
    "        draw_img = None\n",
    "        if self.heatmap_history_count > 0:\n",
    "            labels = self.apply_labels(heat)\n",
    "            bboxes = self.detect_bboxes(labels)\n",
    "            draw_img = self.draw_detected_bboxes(np.copy(image), bboxes)\n",
    "        else:\n",
    "            labels = label(heat)\n",
    "            draw_img = draw_labeled_bboxes(np.copy(image), labels)\n",
    "        if self.debug == True:\n",
    "            self.write_debug_image(draw_img, self.img_count)\n",
    "        return draw_img\n",
    "\n",
    "def process_test_images():\n",
    "    svc, scaler = load_model()\n",
    "    pipeline = Pipeline(svc, scaler, 0.6, 2)\n",
    "    test_images = glob.iglob('./test_images/*.jpg', recursive=True)\n",
    "\n",
    "    test_output = []\n",
    "    for image in test_images:\n",
    "        test_image = cv2.imread(image)\n",
    "        test_output.append(pipeline.process_image(test_image))\n",
    "\n",
    "    for output_image in test_output:\n",
    "        plt.imshow(output_image)\n",
    "        plt.show() \n",
    "        \n",
    "process_test_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Video\n",
    "\n",
    "We run our pipeline on the project video. Here is a link to the output - https://youtu.be/8zgBPJ-fV50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_video():\n",
    "    svc, scaler = load_model()\n",
    "    white_output = 'll_vd_out.mp4'\n",
    "    clip1 = VideoFileClip('lanelines_output.mp4')\n",
    "    pipeline = Pipeline(svc=svc, scaler=scaler, confidence=0.6, \n",
    "                        heat_threshold=2, heatmap_history_count=5)\n",
    "    white_clip = clip1.fl_image(pipeline.process_image)\n",
    "    white_clip.write_videofile(white_output, audio=False)\n",
    "\n",
    "process_video()\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "I think this project is a great way to understand the pros and cons of a computer vision based solution for vehicle detection. In this project we trained a classifier to classify vehicles. While the classifier is generic, all the subsequent logic for sliding windows, heat map etc. are so specific to the images we are testing. So if i ran my pipeline on different video from a different camera angle or different dimensions it won't work.\n",
    "\n",
    "So a more generic approach could be:\n",
    "\n",
    "\n",
    "- Run this pipeline and generate some labeled data for vehicle locations.\n",
    "- Train a CNN to predict vehicle locations (i do not know how to setup an error function that does this, but i think it is doable). This means we do not need to tune window searches.\n",
    "- Once a vehicle is \"locked\", we can use template matching to keep finding that vehicle and need not use a full sliding window approach.\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
